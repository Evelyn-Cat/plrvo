[2024-12-31 10:57:42,358] [INFO] [real_accelerator.py:133:get_accelerator] Setting ds_accelerator to cuda (auto detect)
running stars at 2024-12-31 10:57:43.090520
confirm imported PLRVO
 | tokenizer: BertTokenizer(name_or_path='bert-base-uncased', vocab_size=30522, model_max_length=512, is_fast=False, padding_side='right', truncation_side='right', special_tokens={'unk_token': '[UNK]', 'sep_token': '[SEP]', 'pad_token': '[PAD]', 'cls_token': '[CLS]', 'mask_token': '[MASK]'}, clean_up_tokenization_spaces=True), size: 30522 



 *** dataset sizes: 
train: 392702
valid: 9815
 ***
 | model type: 
<class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>
attention_only: False | randomly_initialize: False
Params to update: 
[
    "bert.embeddings.word_embeddings.weight",
    "bert.embeddings.position_embeddings.weight",
    "bert.embeddings.token_type_embeddings.weight",
    "bert.embeddings.LayerNorm.weight",
    "bert.embeddings.LayerNorm.bias",
    "bert.encoder.layer.0.attention.self.query.weight",
    "bert.encoder.layer.0.attention.self.query.bias",
    "bert.encoder.layer.0.attention.self.key.weight",
    "bert.encoder.layer.0.attention.self.key.bias",
    "bert.encoder.layer.0.attention.self.value.weight",
    "bert.encoder.layer.0.attention.self.value.bias",
    "bert.encoder.layer.0.attention.output.dense.weight",
    "bert.encoder.layer.0.attention.output.dense.bias",
    "bert.encoder.layer.0.attention.output.LayerNorm.weight",
    "bert.encoder.layer.0.attention.output.LayerNorm.bias",
    "bert.encoder.layer.0.intermediate.dense.weight",
    "bert.encoder.layer.0.intermediate.dense.bias",
    "bert.encoder.layer.0.output.dense.weight",
    "bert.encoder.layer.0.output.dense.bias",
    "bert.encoder.layer.0.output.LayerNorm.weight",
    "bert.encoder.layer.0.output.LayerNorm.bias",
    "bert.encoder.layer.1.attention.self.query.weight",
    "bert.encoder.layer.1.attention.self.query.bias",
    "bert.encoder.layer.1.attention.self.key.weight",
    "bert.encoder.layer.1.attention.self.key.bias",
    "bert.encoder.layer.1.attention.self.value.weight",
    "bert.encoder.layer.1.attention.self.value.bias",
    "bert.encoder.layer.1.attention.output.dense.weight",
    "bert.encoder.layer.1.attention.output.dense.bias",
    "bert.encoder.layer.1.attention.output.LayerNorm.weight",
    "bert.encoder.layer.1.attention.output.LayerNorm.bias",
    "bert.encoder.layer.1.intermediate.dense.weight",
    "bert.encoder.layer.1.intermediate.dense.bias",
    "bert.encoder.layer.1.output.dense.weight",
    "bert.encoder.layer.1.output.dense.bias",
    "bert.encoder.layer.1.output.LayerNorm.weight",
    "bert.encoder.layer.1.output.LayerNorm.bias",
    "bert.encoder.layer.2.attention.self.query.weight",
    "bert.encoder.layer.2.attention.self.query.bias",
    "bert.encoder.layer.2.attention.self.key.weight",
    "bert.encoder.layer.2.attention.self.key.bias",
    "bert.encoder.layer.2.attention.self.value.weight",
    "bert.encoder.layer.2.attention.self.value.bias",
    "bert.encoder.layer.2.attention.output.dense.weight",
    "bert.encoder.layer.2.attention.output.dense.bias",
    "bert.encoder.layer.2.attention.output.LayerNorm.weight",
    "bert.encoder.layer.2.attention.output.LayerNorm.bias",
    "bert.encoder.layer.2.intermediate.dense.weight",
    "bert.encoder.layer.2.intermediate.dense.bias",
    "bert.encoder.layer.2.output.dense.weight",
    "bert.encoder.layer.2.output.dense.bias",
    "bert.encoder.layer.2.output.LayerNorm.weight",
    "bert.encoder.layer.2.output.LayerNorm.bias",
    "bert.encoder.layer.3.attention.self.query.weight",
    "bert.encoder.layer.3.attention.self.query.bias",
    "bert.encoder.layer.3.attention.self.key.weight",
    "bert.encoder.layer.3.attention.self.key.bias",
    "bert.encoder.layer.3.attention.self.value.weight",
    "bert.encoder.layer.3.attention.self.value.bias",
    "bert.encoder.layer.3.attention.output.dense.weight",
    "bert.encoder.layer.3.attention.output.dense.bias",
    "bert.encoder.layer.3.attention.output.LayerNorm.weight",
    "bert.encoder.layer.3.attention.output.LayerNorm.bias",
    "bert.encoder.layer.3.intermediate.dense.weight",
    "bert.encoder.layer.3.intermediate.dense.bias",
    "bert.encoder.layer.3.output.dense.weight",
    "bert.encoder.layer.3.output.dense.bias",
    "bert.encoder.layer.3.output.LayerNorm.weight",
    "bert.encoder.layer.3.output.LayerNorm.bias",
    "bert.encoder.layer.4.attention.self.query.weight",
    "bert.encoder.layer.4.attention.self.query.bias",
    "bert.encoder.layer.4.attention.self.key.weight",
    "bert.encoder.layer.4.attention.self.key.bias",
    "bert.encoder.layer.4.attention.self.value.weight",
    "bert.encoder.layer.4.attention.self.value.bias",
    "bert.encoder.layer.4.attention.output.dense.weight",
    "bert.encoder.layer.4.attention.output.dense.bias",
    "bert.encoder.layer.4.attention.output.LayerNorm.weight",
    "bert.encoder.layer.4.attention.output.LayerNorm.bias",
    "bert.encoder.layer.4.intermediate.dense.weight",
    "bert.encoder.layer.4.intermediate.dense.bias",
    "bert.encoder.layer.4.output.dense.weight",
    "bert.encoder.layer.4.output.dense.bias",
    "bert.encoder.layer.4.output.LayerNorm.weight",
    "bert.encoder.layer.4.output.LayerNorm.bias",
    "bert.encoder.layer.5.attention.self.query.weight",
    "bert.encoder.layer.5.attention.self.query.bias",
    "bert.encoder.layer.5.attention.self.key.weight",
    "bert.encoder.layer.5.attention.self.key.bias",
    "bert.encoder.layer.5.attention.self.value.weight",
    "bert.encoder.layer.5.attention.self.value.bias",
    "bert.encoder.layer.5.attention.output.dense.weight",
    "bert.encoder.layer.5.attention.output.dense.bias",
    "bert.encoder.layer.5.attention.output.LayerNorm.weight",
    "bert.encoder.layer.5.attention.output.LayerNorm.bias",
    "bert.encoder.layer.5.intermediate.dense.weight",
    "bert.encoder.layer.5.intermediate.dense.bias",
    "bert.encoder.layer.5.output.dense.weight",
    "bert.encoder.layer.5.output.dense.bias",
    "bert.encoder.layer.5.output.LayerNorm.weight",
    "bert.encoder.layer.5.output.LayerNorm.bias",
    "bert.encoder.layer.6.attention.self.query.weight",
    "bert.encoder.layer.6.attention.self.query.bias",
    "bert.encoder.layer.6.attention.self.key.weight",
    "bert.encoder.layer.6.attention.self.key.bias",
    "bert.encoder.layer.6.attention.self.value.weight",
    "bert.encoder.layer.6.attention.self.value.bias",
    "bert.encoder.layer.6.attention.output.dense.weight",
    "bert.encoder.layer.6.attention.output.dense.bias",
    "bert.encoder.layer.6.attention.output.LayerNorm.weight",
    "bert.encoder.layer.6.attention.output.LayerNorm.bias",
    "bert.encoder.layer.6.intermediate.dense.weight",
    "bert.encoder.layer.6.intermediate.dense.bias",
    "bert.encoder.layer.6.output.dense.weight",
    "bert.encoder.layer.6.output.dense.bias",
    "bert.encoder.layer.6.output.LayerNorm.weight",
    "bert.encoder.layer.6.output.LayerNorm.bias",
    "bert.encoder.layer.7.attention.self.query.weight",
    "bert.encoder.layer.7.attention.self.query.bias",
    "bert.encoder.layer.7.attention.self.key.weight",
    "bert.encoder.layer.7.attention.self.key.bias",
    "bert.encoder.layer.7.attention.self.value.weight",
    "bert.encoder.layer.7.attention.self.value.bias",
    "bert.encoder.layer.7.attention.output.dense.weight",
    "bert.encoder.layer.7.attention.output.dense.bias",
    "bert.encoder.layer.7.attention.output.LayerNorm.weight",
    "bert.encoder.layer.7.attention.output.LayerNorm.bias",
    "bert.encoder.layer.7.intermediate.dense.weight",
    "bert.encoder.layer.7.intermediate.dense.bias",
    "bert.encoder.layer.7.output.dense.weight",
    "bert.encoder.layer.7.output.dense.bias",
    "bert.encoder.layer.7.output.LayerNorm.weight",
    "bert.encoder.layer.7.output.LayerNorm.bias",
    "bert.encoder.layer.8.attention.self.query.weight",
    "bert.encoder.layer.8.attention.self.query.bias",
    "bert.encoder.layer.8.attention.self.key.weight",
    "bert.encoder.layer.8.attention.self.key.bias",
    "bert.encoder.layer.8.attention.self.value.weight",
    "bert.encoder.layer.8.attention.self.value.bias",
    "bert.encoder.layer.8.attention.output.dense.weight",
    "bert.encoder.layer.8.attention.output.dense.bias",
    "bert.encoder.layer.8.attention.output.LayerNorm.weight",
    "bert.encoder.layer.8.attention.output.LayerNorm.bias",
    "bert.encoder.layer.8.intermediate.dense.weight",
    "bert.encoder.layer.8.intermediate.dense.bias",
    "bert.encoder.layer.8.output.dense.weight",
    "bert.encoder.layer.8.output.dense.bias",
    "bert.encoder.layer.8.output.LayerNorm.weight",
    "bert.encoder.layer.8.output.LayerNorm.bias",
    "bert.encoder.layer.9.attention.self.query.weight",
    "bert.encoder.layer.9.attention.self.query.bias",
    "bert.encoder.layer.9.attention.self.key.weight",
    "bert.encoder.layer.9.attention.self.key.bias",
    "bert.encoder.layer.9.attention.self.value.weight",
    "bert.encoder.layer.9.attention.self.value.bias",
    "bert.encoder.layer.9.attention.output.dense.weight",
    "bert.encoder.layer.9.attention.output.dense.bias",
    "bert.encoder.layer.9.attention.output.LayerNorm.weight",
    "bert.encoder.layer.9.attention.output.LayerNorm.bias",
    "bert.encoder.layer.9.intermediate.dense.weight",
    "bert.encoder.layer.9.intermediate.dense.bias",
    "bert.encoder.layer.9.output.dense.weight",
    "bert.encoder.layer.9.output.dense.bias",
    "bert.encoder.layer.9.output.LayerNorm.weight",
    "bert.encoder.layer.9.output.LayerNorm.bias",
    "bert.encoder.layer.10.attention.self.query.weight",
    "bert.encoder.layer.10.attention.self.query.bias",
    "bert.encoder.layer.10.attention.self.key.weight",
    "bert.encoder.layer.10.attention.self.key.bias",
    "bert.encoder.layer.10.attention.self.value.weight",
    "bert.encoder.layer.10.attention.self.value.bias",
    "bert.encoder.layer.10.attention.output.dense.weight",
    "bert.encoder.layer.10.attention.output.dense.bias",
    "bert.encoder.layer.10.attention.output.LayerNorm.weight",
    "bert.encoder.layer.10.attention.output.LayerNorm.bias",
    "bert.encoder.layer.10.intermediate.dense.weight",
    "bert.encoder.layer.10.intermediate.dense.bias",
    "bert.encoder.layer.10.output.dense.weight",
    "bert.encoder.layer.10.output.dense.bias",
    "bert.encoder.layer.10.output.LayerNorm.weight",
    "bert.encoder.layer.10.output.LayerNorm.bias",
    "bert.encoder.layer.11.attention.self.query.weight",
    "bert.encoder.layer.11.attention.self.query.bias",
    "bert.encoder.layer.11.attention.self.key.weight",
    "bert.encoder.layer.11.attention.self.key.bias",
    "bert.encoder.layer.11.attention.self.value.weight",
    "bert.encoder.layer.11.attention.self.value.bias",
    "bert.encoder.layer.11.attention.output.dense.weight",
    "bert.encoder.layer.11.attention.output.dense.bias",
    "bert.encoder.layer.11.attention.output.LayerNorm.weight",
    "bert.encoder.layer.11.attention.output.LayerNorm.bias",
    "bert.encoder.layer.11.intermediate.dense.weight",
    "bert.encoder.layer.11.intermediate.dense.bias",
    "bert.encoder.layer.11.output.dense.weight",
    "bert.encoder.layer.11.output.dense.bias",
    "bert.encoder.layer.11.output.LayerNorm.weight",
    "bert.encoder.layer.11.output.LayerNorm.bias",
    "bert.pooler.dense.weight",
    "bert.pooler.dense.bias",
    "classifier.weight",
    "classifier.bias"
]
Number of differentiable params: 109.485 million
per_example_max_grad_norm is 1.27586206896552 [self.max_grad_norm]
{'a_G': 1, 'G_k': 53.0653266331658, 'G_theta': 0.0158332201548282, 'distributions': ['Gamma'], 'steps': 197, 'eps': 0.922877680843546, 'PLRV-Dist': 1.21305963650293, 'delta': 7.42401520438314e-06, 'C': 1.27586206896552, 'sample_rate': 0.0152043831385767, 'Gauss_distortion': 1.37056798758602, 'noise_multiplier': 1.3520751953125, 'sigma': 1.725061456088366}
0
0
privacy_args: 
{
    "noise_type": "PLRVO",
    "config_idx": 14,
    "per_example_max_grad_norm": 0.1,
    "noise_multiplier": 0,
    "target_epsilon": 0,
    "target_delta": 1.2732300828618139e-06,
    "non_private": false,
    "accounting_mode": "rdp",
    "clipping_mode": "ghost"
}
{'contradiction': 'no', 'entailment': 'yes', 'neutral': 'maybe'}
*cls**sent-_0*?*mask*,*+sentl_1**sep+*
2: -1
2: 2311 7 3.0
metrics: 
{
    "eval_loss": 1.1492607593536377,
    "eval_mnli/acc": 0.32572592969943964
}
dev objective eval_mnli/acc: 0.32572592969943964
metrics: 
{
    "eval_loss": 1.7785155773162842,
    "eval_mnli/acc": 0.7352012226184411
}
dev objective eval_mnli/acc: 0.7352012226184411
metrics: 
{
    "eval_loss": 1.502050757408142,
    "eval_mnli/acc": 0.788792664289353
}
dev objective eval_mnli/acc: 0.788792664289353
metrics: 
{
    "eval_loss": 1.466524600982666,
    "eval_mnli/acc": 0.7959246051961284
}
dev objective eval_mnli/acc: 0.7959246051961284
metrics: 
{
    "eval_loss": 1.4294766187667847,
    "eval_mnli/acc": 0.8099847172694855
}
dev objective eval_mnli/acc: 0.8099847172694855
metrics: 
{
    "eval_loss": 1.4260154962539673,
    "eval_mnli/acc": 0.8107997962302598
}
dev objective eval_mnli/acc: 0.8107997962302598
metrics: 
{
    "eval_loss": 1.357438564300537,
    "eval_mnli/acc": 0.8201732042791645
}
dev objective eval_mnli/acc: 0.8201732042791645
metrics: 
{
    "eval_loss": 1.4737242460250854,
    "eval_mnli/acc": 0.8195618950585838
}
dev objective eval_mnli/acc: 0.8195618950585838
metrics: 
{
    "eval_loss": 1.489201307296753,
    "eval_mnli/acc": 0.8204788588894549
}
dev objective eval_mnli/acc: 0.8204788588894549
metrics: 
{
    "eval_loss": 1.4713252782821655,
    "eval_mnli/acc": 0.8225165562913908
}
dev objective eval_mnli/acc: 0.8225165562913908
metrics: 
{
    "eval_loss": 1.4143297672271729,
    "eval_mnli/acc": 0.83015792154865
}
dev objective eval_mnli/acc: 0.83015792154865
running time: 19610.12 seconds... [about 5.447254507078065 hours]
no
learning rate is 5e-05
Running command:

CUDA_VISIBLE_DEVICES=0 python -m classification.run_classification_final   --per_device_train_batch_size 170   --task_name mnli --model_name_or_path bert-base-uncased   --noise_type PLRVO --config_idx 14   --non_private no --output_dir /mnt/backup/home/qiy22005/PRO/plrvo/scripts/results/classification/mnli/PLRVO.bert-base-uncased.config_14 --overwrite_output_dir   --gradient_accumulation_steps 7 --num_train_epochs 3   --learning_rate 5e-05 --clipping_mode ghost --few_shot_type finetune --data_dir classification/data/original/MNLI   --seed 42 --randomly_initialize no --store_grads no --template *cls**sent-_0*?*mask*,*+sentl_1**sep+*   --attention_only no --static_lm_head no --static_embedding no   --eval_steps 100 --eval_spectrum no --max_spectrum_batches 2 --max_lanczos_iter 2   --weight_decay 0 --lr_decay no --adam_epsilon 1e-08 --max_seq_len 256 --per_device_eval_batch_size 100   --evaluation_strategy steps --evaluate_before_training True --do_train --do_eval --num_sample 1 --num_k 1   --first_sent_limit 200 --other_sent_limit 200 --truncate_head yes 
